The reliance of popular programming languages such as Python and JavaScript on centralized package repositories and open-source software has created a new type of threat to the software supply chain: package hallucinations. These hallucinations, which arise from fact-conflicting errors when generating code using Large Language Models (LLMs), represent a novel form of package confusion attack that poses a critical threat to the integrity of the software supply chain.

Researchers have conducted a rigorous and comprehensive evaluation of package hallucinations across different programming languages, settings, and parameters, exploring how different configurations of LLMs affect the likelihood of generating erroneous package recommendations and identifying the root causes of this phenomenon. Using 16 different popular code generation models, across two programming languages and two unique prompt datasets, they collected 576,000 code samples which they analyzed for package hallucinations. Their findings reveal that 19.7% of generated packages across all the tested LLMs are hallucinated, including a staggering 205,474 unique examples of hallucinated package names, further underscoring the severity and pervasiveness of this threat.

The study also implemented and evaluated mitigation strategies based on Retrieval Augmented Generation (RAG), self-detected feedback, and supervised fine-tuning. These techniques demonstrably reduced package hallucinations, with hallucination rates for one model dropping below 3%. While the mitigation efforts were effective in reducing hallucination rates, the study reveals that package hallucinations are a systemic and persistent phenomenon that pose a significant challenge for code generating LLMs.

The reliance of popular programming languages such as Python and JavaScript on centralized package repositories and open-source software has created a new type of threat to the software supply chain: package hallucinations. These hallucinations, which arise from fact-conflicting errors when generating code using Large Language Models (LLMs), represent a novel form of package confusion attack that poses a critical threat to the integrity of the software supply chain.

In this study, we conducted a systematic analysis of the frequency and nature of package hallucinations across a variety of code generation LLMs operating under a diverse set of model settings and parameters. We rigorously analyzed and quantified the impact of these hallucinations on the security of the generated code (vis-à-vis package confusion attacks) and evaluated several mitigation strategies. We specifically made the following novel contributions:

• Quantifying the incidence and origins of package hallucinations in code generating LLMs: We comprehensively analyzed the prevalence of package hallucinations in Python and JavaScript code generated by popular publicly-available commercial and open-source LLMs. We also examined the common behaviors in LLMs that lead to package hallucinations, including hallucination repetition, output verbosity, and the ability of models to detect their own hallucinations.

• Analyzing the effect of model settings and training data on package hallucinations: We further studied how specific model settings, such as training data recency, model temperature, and decoding strategies affect the occurrence and nature of package hallucinations.

• Characterizing the generated hallucinated