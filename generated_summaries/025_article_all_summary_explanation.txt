Package hallucinations, a phenomenon where Large Language Models (LLMs) generate code that recommends or contains non-existent software packages, pose a significant threat to software security. In this study, we comprehensively analyze the frequency and nature of package hallucinations across various code generation LLMs, exploring their impact on code security and evaluating mitigation strategies. Our findings reveal that 19.7% of generated packages are fictitious, highlighting the need for robust mitigation techniques to ensure the reliability and security of AI-assisted software development.

We conducted a rigorous evaluation of 16 popular code generation models, across two programming languages and two unique prompt datasets, collecting 576,000 code samples. Our analysis reveals that package hallucinations are a systemic and persistent phenomenon, with hallucination rates dropping below 3% using Retrieval Augmented Generation (RAG) and supervised fine-tuning techniques.

Our study highlights the importance of addressing package hallucinations to enhance software security. We propose and evaluate several mitigation strategies, including RAG, self-detected feedback, and supervised fine-tuning. These techniques demonstrate promise in reducing package hallucinations, underscoring the need for continued research and development to ensure the safe and reliable deployment of AI-assisted software development tools.

By understanding the underlying causes of package hallucinations and developing effective mitigation strategies, we can enhance the security and reliability of AI-assisted software development, ensuring the responsible adoption of these powerful systems in critical domains like software engineering.

Keywords: Package hallucinations, Large Language Models, Code generation, Software security, Mitigation strategies.
