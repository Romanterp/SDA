The paper investigates the phenomenon of package hallucinations in code-generating Large Language Models (LLMs), which can lead to a new type of threat to the software supply chain. The study finds that 19.7% of generated packages are fictitious, posing a critical threat to software security through package confusion attacks.

The researchers conducted a comprehensive evaluation of package hallucinations across different programming languages, settings, and parameters, using 16 different popular code generation models. They collected 576,000 code samples and analyzed them for package hallucinations, revealing that the phenomenon is systemic and persistent.

The study identified several key behavioral patterns that lead to package hallucinations, including hallucination repetition, output verbosity, and the ability of models to detect their own hallucinations. The researchers also characterized the generated hallucinated packages, including their semantic similarity to popular packages, the occurrence of package recommendations from other programming languages, and package persistence.

To mitigate package hallucinations, the researchers proposed and evaluated several techniques, including Retrieval Augmented Generation (RAG), self-detected feedback, and supervised fine-tuning. These techniques were found to be effective in reducing hallucination rates, with one model's hallucination rate dropping below 3%.

The study highlights the importance of addressing package hallucinations to enhance the reliability and security of AI-assisted software development. Future work includes identifying and mitigating the underlying causes of package hallucinations, developing more sophisticated mitigation strategies, and integrating real-time feedback mechanisms to dynamically adjust model output.

The findings of this study have significant implications for the software development industry, as they demonstrate the need for robust and reliable code generation models that can effectively mitigate package hallucinations. By understanding the causes and consequences of package hallucinations, developers can take steps to improve the security and integrity of their software systems. ### 1. Introduction
The reliance of popular programming languages such as Python and JavaScript on centralized package repositories and open-source software, combined with the emergence of code-generating Large Language Models (LLMs), has created a new type of threat to the software supply chain: package hallucinations. These hallucinations, which arise from fact-conflicting errors when generating code using LLMs, represent a novel form of package confusion attack that poses a critical threat to the integrity of the software supply chain.
The researchers conducted a comprehensive evaluation of package hallucinations across different programming languages, settings, and parameters, using 16 different popular code generation models. They collected 576,000 code samples and analyzed them for package hallucinations, revealing that the phenomenon is systemic and persistent.