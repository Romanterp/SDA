Imagine trying to measure the height of a growing tree. You might use a ruler, but how reliable would that measurement be if you only took it once? This paper explores a similar challenge in the world of artificial intelligence (AI). AI is a type of computer program that can perform tasks that usually require human intelligence, such as understanding language, recognizing images, and making decisions. Evaluating the capabilities of these AI programs is crucial, and a key tool for this is the benchmark – a set of tasks that test the AI's performance.

Just like that single ruler measurement, benchmark scores can be misleading if we don't consider their variability. This variability, called variance, can be caused by different factors, such as the way the task is phrased or the random starting point used when training the AI. The authors of this paper studied variance in AI benchmarks on a large scale, looking at 13 different benchmarks and 280 different AI models.

Their findings are three-fold. Firstly, they provide a reference point for how much variance to expect from different benchmarks. This is helpful for researchers comparing AI models, as it allows them to assess whether observed differences in performance are actually meaningful. Secondly, they suggest ways to reduce variance for smaller AI models on specific tasks. For instance, they found that rephrasing a choice-based task can lead to more consistent results.

Thirdly, the paper discourages the use of certain methods borrowed from human testing to reduce variance in AI models. While these methods might be effective for evaluating human performance, they don't seem to work well for AI models. Instead, the authors recommend techniques specifically designed for AI models, such as using continuous scoring metrics or tasks where the AI has to fill in missing words.

Overall, this research sheds light on a critical but often overlooked aspect of AI evaluation – variance. By quantifying this variability and exploring methods to reduce it, the authors pave the way for more reliable comparisons of AI models, ultimately helping us understand and improve these powerful tools.