Neural networks have shown remarkable performance in various tasks, but understanding how they acquire relevant features from data remains elusive. This research paper investigates the mechanisms underlying the transition between two distinct learning regimes: the lazy regime, where networks remain close to their initialization, and the rich regime, characterized by efficient feature learning.

The study aims to elucidate how layer-specific initialization variances and learning rates influence the extent of feature learning. The researchers derived exact solutions for a minimal linear model that transitions between lazy and rich learning. Their analysis revealed that the initialization geometry, determined by these layer-specific parameters, governs the learning regime through conserved quantities that constrain the learning trajectories.

Notably, the researchers found that unbalanced initializations, where early layers learn faster (upstream initialization), can accelerate rich learning and promote efficient feature acquisition. This contrasts with the common belief that balanced initializations are necessary for rich learning in linear networks.

The study extended the analysis to more complex linear and nonlinear models, reinforcing the importance of the initialization geometry. Experiments provided evidence that upstream initializations drive feature learning in deep networks, enhance interpretability, reduce sample complexity for hierarchical data, and improve learning modular arithmetic.

While the theory's limitations lie in the difficulty of extending it to deeper nonlinear networks and the omission of stochastic effects, the researchers highlight the potential for further exploration of unbalanced initializations to optimize efficient feature learning. Understanding how the learning speed profile across layers impacts feature learning, inductive biases, and generalization is an important direction for future work.

Overall, this research contributes to our theoretical understanding of feature learning in neural networks and highlights the significance of the initialization geometry in determining the learning regime. The findings motivate further investigation into unbalanced initializations as a means to enhance the efficiency of feature acquisition and improve the performance of deep learning models.