STAT: A Novel Approach to Prune Transformer Models Without Fine-tuning

Transformers, a type of neural network architecture, have become ubiquitous in natural language processing tasks. However, the best-performing transformer models are often prohibitively large, necessitating compression techniques like pruning to make them more accessible. This paper introduces STAT, a novel algorithm that can efficiently prune transformer models by eliminating attention heads and neurons without requiring any fine-tuning.

The researchers present a method that leverages matrix factorizations to calculate corrections to the weights of the next layer, preserving the network's accuracy while significantly reducing its size. STAT eliminates redundant components from the network by analyzing the activations of a small set of unlabeled data examples, identifying the least important attention heads and neurons to prune.

The key advantages of STAT lie in its simplicity and efficiency. The entire compression process takes only minutes for models like BERT and under three hours for larger models with billions of parameters, using a single GPU. Despite its speed, STAT outperforms existing gradient-free pruning methods and even competes with techniques that involve extensive fine-tuning.

The researchers demonstrate STAT's effectiveness on various transformer models, including BERT, DistilBERT, and the large Llama-2 model with 7 billion parameters. They evaluate the compressed models on benchmarks such as GLUE, SQuAD, and WikiText2, achieving impressive performance while significantly reducing the models' computational footprints.

While STAT presents a promising solution for transformer model compression, the researchers acknowledge some limitations. Notably, the error metric computation for different network architectures may require slight adaptations, and further exploration is needed to fully understand the impact of layer normalization placement on the method's sensitivity.

Overall, STAT offers a practical and efficient approach to compress transformer models without sacrificing accuracy, making state-of-the-art models more accessible to a broader range of users and applications. This research paves the way for more efficient and widespread deployment of transformer-based language models, facilitating their adoption in various domains.