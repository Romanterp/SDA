Abstract
While the impressive performance of modern neural networks is often attributed
to their capacity to efficiently extract task-relevant features from data, the mech-
anisms underlying this rich feature learning regime remain elusive, with much
of our theoretical understanding stemming from the opposing lazy regime. In
this work, we derive exact solutions to a minimal model that transitions between
lazy and rich learning, precisely elucidating how unbalanced layer-specific ini-
tialization variances and learning rates determine the degree of feature learning.
Our analysis reveals that they conspire to influence the learning regime through
a set of conserved quantities that constrain and modify the geometry of learning
trajectories in parameter and function space. We extend our analysis to more
complex linear models with multiple neurons, outputs, and layers and to shallow
nonlinear networks with piecewise linear activation functions. In linear networks,
rapid feature learning only occurs with balanced initializations, where all layers
learn at similar speeds. While in nonlinear networks, unbalanced initializations that
promote faster learning in earlier layers can accelerate rich learning. Through a
series of experiments, we provide evidence that this unbalanced rich regime drives
feature learning in deep finite-width networks, promotes interpretability of early
layers in CNNs, reduces the sample complexity of learning hierarchical data, and
decreases the time to grokking in modular arithmetic. Our theory motivates further
exploration of unbalanced initializations to enhance efficient feature learning.
1 Introduction
Deep learning has transformed machine learning, demonstrating remarkable capabilities in a myriad
of tasks ranging from image recognition to natural language processing. It’s widely believed that
the impressive performance of these models lies in their capacity to efficiently extract task-relevant
features from data. However, understanding this feature acquisition requires unraveling a complex in-
terplay between datasets, network architectures, and optimization algorithms. Within this framework,
two distinct regimes, determined at initialization, have emerged: the lazy and the rich.
Lazy regime. Various investigations have revealed a notable phenomenon in overparameterized neural
networks, where throughout training the networks remain close to their linearization [ 1 , 2, 3, 4, 5 ].
Seminal work by Jacot et al. [6], demonstrated that in the infinite-width limit, the Neural Tangent
Kernel (NTK), which describes the evolution of the neural network through training, converges
to a deterministic limit. Consequently, the network learns a solution akin to kernel regression
with the NTK matrix. Termed the lazy or kernel regime, this domain has been characterized by a
deterministic NTK [ 6, 7], convex dynamics with minimal movement in parameter space [ 8 ], static
hidden representations, exponential learning curves, and implicit biases aligned with a reproducing
kernel Hilbert space (RKHS) norm [9]. However, Chizat et al. [8] challenged this understanding,
Figure 1: Unbalanced initializations lead to rapid rich learning and generalization. We follow
the experimental setup used in Fig. 1 of Chizat et al. [8] – a wide two-layer student ReLU network
f (x; θ) = Ph
i=1 ai max{0, w⊺
i x} trained on a dataset generated from a narrow two-layer teacher
ReLU network. The student parameters are initialized as wi ∼ Unif(Sd−1( τ
α )) and ai = ±ατ
such that τ > 0 controls the scale of the function at initialization, while α > 0 controls the
geometry, the relative magnitude of the first and second layer parameters. The conserved quantity is
δ = τ 2(α2 − α−2). (a) Shows the training trajectories of |ai|wi (color denotes sgn(ai)) when d = 2
for four different settings of τ, δ. The left plot confirms that small scale leads to rich and large scale to
lazy. The right plot shows that even at small scale, the initialization geometry can move the network
between rich and lazy as well. Here an upstream initialization δ > 0 shows striking alignment to the
teacher (dotted lines), while a downstream initialization δ < 0 shows no alignment. (b) Shows the
test loss and kernel distance from initialization computed through training over a sweep of τ and δ
when d = 100. Lazy learning happens when τ is large, rich learning happens when τ is small, and
rapid rich learning happens when both τ is small and δ is large – an upstream initialization. This
initialization also leads to the smallest test loss. See Fig. 10 in Appendix D.1 for supporting figures.
asserting that the lazy regime isn’t a product of the infinite-width architecture, but is contingent
on the scale of the network at initialization. They demonstrated that given any finite-width model
f (x; θ) whose output is zero at initialization, a scaled version of the model τ f (x; θ) will enter the
lazy regime as the scale τ diverges. However, they also noted that these scaled models often perform
worse in test error. While the lazy regime offers insights into the network’s convergence to a global
minimum, it does not fully capture the generalization capabilities of neural networks trained with
standard initializations. It is thus widely believed that a different regime, driven by small or vanishing
initializations, underlies the many successes of neural networks.
Rich regime. In contrast to the lazy regime, the rich or feature-learning or active regime is distin-
guished by a learned NTK that evolves through training, non-convex dynamics traversing between
saddle points [ 10, 11, 12 ], sigmoidal learning curves, and simplicity biases such as low-rankness [ 13 ]
or sparsity [ 14 ]. Yet, the exact characterization of rich learning and the features it learns frequently
depends on the specific problem at hand, with its definition commonly simplified as what it is not: lazy.
Recent analyses have shown that beyond scale, other aspects of the initialization can substantially
impact the extent of feature learning, such as the effective rank [15 ], layer-specific initialization
variances [ 16, 17, 18 ], and large learning rates [ 19, 20, 21, 22 ]. Azulay et al. [9] demonstrated that in
two-layer linear networks, the relative difference in weight magnitudes between the first and second
layer, termed the initialization geometry in our work, can impact feature learning, with balanced
initializations yielding rich learning dynamics, while unbalanced ones tend to induce lazy dynamics.
However, as shown in Fig. 1, for nonlinear networks unbalanced initializations can induce both rich
and lazy dynamics, creating a complex phase portrait of learning regimes influenced by both scale and
geometry. Building on these observations, our study aims to precisely understand how layer-specific
initialization variances and learning rates determine the transition between lazy and rich learning
in finite-width networks. Moreover, we endeavor to gain insights into the inductive biases of both
regimes, and the transition between them, during training and at interpolation, with the ultimate goal
of elucidating how the rich regime acquires features that facilitate generalization.
Our contributions. Our work begins with an exploration of the two-layer single-neuron linear
network proposed by Azulay et al. [9] as a minimal model displaying both lazy and rich learning. By
employing a combination of hyperbolic and spherical coordinate transformations, we derive exact
solutions for the gradient flow dynamics with layer-specific learning rates under all initializations.
Alongside recent work by Xu and Ziyin [23]1, our analysis stands out as one of the few analytically
tractable models for the transition between lazy and rich learning in a finite-width network, marking
a notable contribution to the field. Our analysis reveals that the layer-specific initialization variances
and learning rates, which we collectively refer to as the initialization geometry, conspire to influence
the learning regime through a simple set of conserved quantities that constrain the geometry of
learning trajectories. Additionally, it reveals that a crucial aspect of the initialization geometry
overlooked in prior analysis is the directionality. While a balanced initialization results in all layers
learning at similar rates, an unbalanced initialization can cause faster learning in either earlier layers,
referred to as an upstream initialization, or later layers, referred to as a downstream initialization. Due
to the depth-dependent expressivity of layers in a network, upstream and downstream initializations
often exhibit fundamentally distinct learning trajectories. We extend our analysis of the initialization
geometry developed in the single-neuron model to more complex linear models with multiple neurons,
outputs, and layers and to two-layer nonlinear networks with piecewise linear activation functions.
We find that in linear networks, rapid rich learning can only occur with balanced initializations, while
in nonlinear networks, upstream initializations can actually accelerate rich learning. Finally, through
a series of experiments, we provide evidence that upstream initializations drive feature learning in
deep finite-width networks, promote interpretability of early layers in CNNs, reduce the sample
complexity of learning hierarchical data, and decrease the time to grokking in modular arithmetic.
Notation. In this work, we consider a feedforward network f (x; θ) : Rd → R parameterized by θ ∈
Rp. The network is trained by gradient flow ˙θi = −ηθi ∇θi L(θ), with an initialization θ0 and layer-
specific learning rate ηθi > 0 ∈ Rp, to minimize the mean squared error L(θ) = 1
2
Pn
i=1(f (xi; θ) −
yi)2 computed over a dataset {(x1, y1), . . . , (xn, yn)} of size n. We denote the training data matrix
as X ∈ Rn×d with rows xi ∈ Rd and the label vector as y ∈ Rn. The network’s output f (x; θ)
evolves according to the differential equation, ∂tf (x; θ) = − Pn
i=1 Θ(x, xi; θ)(f (xi; θ) − yi),
where Θ(x, x′; θ) : Rd × Rd → R is the Neural Tangent Kernel (NTK), defined as Θ(x, x′; θ) =Pp
i=1 ηθi ∂θi f (x; θ)∂θi f (x′; θ). The NTK quantifies how one gradient step with data point x′ affects
the evolution of the networks’s output evaluated at another data point x. When ηθi is shared
by all parameters, the NTK is the kernel associated with the feature map ∇θ f (x; θ) ∈ Rp. We
also define the NTK matrix K ∈ Rn×n, which is computed across the training data such that
Kij = Θ(xi, xj ; θ). The NTK matrix evolves from its initialization K0 to convergence K∞ through
training. Lazy and rich learning exist on a spectrum, with the extent of this evolution serving as
the distinguishing factor. Various studies have proposed different metrics to track the evolution of
the NTK matrix [ 24 , 25, 26 , 27]. In this work, we use kernel distance [ 25], defined as S(t1, t2) =
1 − ⟨Kt1 , Kt2 ⟩/ (∥Kt1 ∥F ∥Kt2 ∥F ), which is a scale invariant measure of similarity between the NTK
at two points in time. In the lazy regime S(0, t) ≈ 0, while in the rich regime 0 ≪ S(0, t) ≤ 1.

6 Conclusion
In this work, we derived exact solutions to a minimal model that can transition between lazy and rich
learning to precisely elucidate how unbalanced layer-specific initialization variances and learning
rates determine the degree of feature learning. We further extended our analysis to wide and deep
linear networks and shallow piecewise linear networks. We find through theory and empirics that
unbalanced initializations, which promote faster learning at earlier layers, can actually accelerate rich
learning. Limitations. The primary limitation lies in the difficulty to extend our theory to deeper
nonlinear networks. In contrast to linear networks, where additional symmetries simplify dynamics,
nonlinear networks require consideration of the activation pattern’s impact on subsequent layers. One
potential solution involves leveraging the path framework used in Saxe et al. [70]. Another limitation
is our omission of discretization and stochastic effects of SGD, which disrupt the conservation laws
central to our study and introduce additional simplicity biases [ 71 , 72, 73 ]. Future work. Our
theory encourages further investigation into unbalanced initializations to optimize efficient feature
learning. In deep networks there are many ways to be unbalanced beyond upstream and downstream.
Understanding how the learning speed profile across layers impacts feature learning, inductive biases,
and generalization is an important direction for future work.